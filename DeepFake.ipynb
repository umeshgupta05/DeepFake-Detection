
from inference_sdk import InferenceHTTPClient
import base64
import os
import json

# Initialize the inference client
CLIENT = InferenceHTTPClient(
    api_url="https://detect.roboflow.com",
    api_key="K1aE1fYjCXZQSaMsW6ak"
)

# Directory containing test images
test_dir = "/content/drive/MyDrive/dataset/test"

# Output JSON file (Replace 'teamname' with your actual team name)
output_json_path = "/content/drive/MyDrive/dataset/saiumeshgupta_predictions.json"

# List to store formatted results
results_list = []

# Get all image files in the test directory
image_files = [f for f in os.listdir(test_dir) if f.endswith(('.png', '.jpg', '.jpeg'))]
image_files.sort()  # Ensure consistent order

# Process each image in the directory
for image_file in image_files:
    image_path = os.path.join(test_dir, image_file)

    try:
        # Read and encode the image in base64
        with open(image_path, "rb") as img_file:
            encoded_string = base64.b64encode(img_file.read()).decode("utf-8")

        # Perform inference using the API
        result = CLIENT.infer(encoded_string, model_id="deepfake-ebv1o/1")

        # Extract the top prediction (API always provides top > 0.5)
        top_prediction = result.get("top", "unknown")

        # Assign final prediction
        final_prediction = "fake" if top_prediction == "fake_cifake_images" else "real"

        # Extract numeric index from filename (e.g., "1.png" -> 1)
        index = int(os.path.splitext(image_file)[0])

        # Store formatted result
        formatted_result = {
            "index": index,
            "prediction": final_prediction
        }
        results_list.append(formatted_result)

        print(f"Processed {image_path}: {formatted_result}")

    except Exception as e:
        print(f"Error processing {image_path}: {str(e)}")

# Save results in JSON format
with open(output_json_path, "w") as json_file:
    json.dump(results_list, json_file, indent=2)

print(f"Inference results saved to {output_json_path}")
